from collections import defaultdict
import csv
import json
from logging import Logger
import os
import sys
from typing import Callable, Dict, List, Tuple
import subprocess

import numpy as np
import pandas as pd
import torch
from run_training import run_training
from args import TrainArgs
from utils import create_logger, makedirs, timeit, set_seed

import scanpy as sc
from data.utils import train_valid_test


@timeit(logger_name='train_logger')
def cross_validate(rank, world_size, train_func, parallel=True):
    """
    Runs k-fold cross-validation.

    For each of k splits (folds) of the data, trains and tests a model on that split
    and aggregates the performance across folds.

    :param rank: Rank of the current process.
    :param world_size: Total number of processes.
    :param train_func: Function which runs training.
    :param parallel: Whether to run in parallel (using multiple GPUs) or not.
    :return: A tuple containing the mean and standard deviation performance across folds.
    """
    
    arguments = [
        '--epochs', '200',
        '--dataset_type', 'regression',
        '--data_path', '/home/chaewon215/research/PRnet/dataset/Lincs_L1000.h5ad',
        '--save_dir', 'checkpoints',
        '--comp_embed_model', 'molformer',
        '--metric', 'avg_gene_pearson',
    ]

    args = TrainArgs().parse_args(arguments)
    
    args.split_keys = ['drug_split_4']
    args.rank = rank
    args.world_size = world_size
    args.batch_size = 128 * world_size
    args.loss_function = 'hybrid_loss'
    args.lambda_kl = 0.001
    args.parallel = parallel
    
    args.save_smiles_splits = False
    args.save_preds = True
    args.train_data_size = 0
    
    args.mode = 'pred_mu_v' # 'pred_mu_var' or 'pred_mu_v'
    
    
    if not args.parallel:
        args.device = torch.device('cuda', 0)
    else:
        args.device = torch.device('cuda', rank)


    # Set up logger
    logger = create_logger(name='train_logger', save_dir=args.save_dir, quiet=args.quiet)
    if logger is not None:
        debug, info = logger.debug, logger.info
    else:
        debug = info = print

    # Initialize relevant variables
    save_dir = args.save_dir

    # Print command line
    debug('Command line')
    debug(f'python {" ".join(sys.argv)}')

    # Print args
    debug('Args')
    debug(args)

    debug(f"Setting seeds...: {args.seed}")
    set_seed(args.seed)

    # Save args
    makedirs(args.save_dir)
    try:
        args.save(os.path.join(args.save_dir, 'args.json'))
    except subprocess.CalledProcessError:
        debug('Could not write the reproducibility section of the arguments to file, thus omitting this section.')
        args.save(os.path.join(args.save_dir, 'args.json'), with_reproducibility=False)

    # Get data
    debug('Loading Ann data...')
    adata = sc.read(args.data_path)
    
    args.adata_var_names = adata.var_names
    args.obs_key = 'cov_drug_name'
    args.task_names = args.adata_var_names

    # Run training on different random seeds for each fold
    all_scores = defaultdict(list)
    for index in range(len(args.split_keys)):
        
        split_key = args.split_keys[index]
        info(f'Fold {index}')
    
        args.save_dir = os.path.join(save_dir, f'fold_{index}')
        makedirs(args.save_dir)

        train_adata, valid_adata, test_adata = train_valid_test(adata, split_key=split_key, sample=True)
        debug(f'Number of data points = {len(adata)}')
        debug(f'--- Training data points = {len(train_adata)}')
        debug(f'--- Validation data points = {len(valid_adata)}')
        debug(f'--- Test data points = {len(test_adata)}')
        
        debug(f'Number of tasks = {args.num_tasks}')
        
        args.train_data_size = len(train_adata)
        
        data = {
            'train': train_adata,
            'valid': valid_adata,
            'test': test_adata
            }
        
        model_scores = train_func(args, data, logger)


        for metric, scores in model_scores.items():
            all_scores[metric].append(scores)
            
    all_scores = dict(all_scores)
    
    # Aggregate scores across folds
    mean_scores = {} 
    std_scores = {}
    for metric, scores in all_scores.items():
        mean_scores[metric] = np.mean(scores)
        std_scores[metric] = np.std(scores)
        
    info('Overall cross-validation results:')
    for metric in mean_scores.keys():
        info(f'{metric}: {mean_scores[metric]} Â± {std_scores[metric]}')
        
    return mean_scores, std_scores
